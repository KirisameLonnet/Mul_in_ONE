<div style="text-align: center; padding: 80px 20px; page-break-after: always;">
  <div style="border-top: 3px solid #333; width: 80%; margin: 0 auto 60px;"></div>

<h1 style="font-size: 28px; color: #333; margin: 40px 0 20px; font-weight: normal; letter-spacing: 2px; text-align: center;">
    2025 年上海大学人工智能创新大赛
  </h1>

<h2 style="font-size: 48px; color: #333; margin: 40px 0 80px; font-weight: bold; letter-spacing: 4px; text-align: center;">
    作品研究报告
  </h2>

<div style="text-align: left; width: 70%; margin: 100px auto; font-size: 18px; line-height: 2.5;">
    <p style="margin: 20px 0;">
      <span style="display: inline-block; width: 120px; font-weight: bold;">作品名称：</span>
      <span style="border-bottom: 1px solid #333; padding-bottom: 2px;">多智能体协同 RAG 增强角色扮演系统</span>
    </p>
    <p style="margin: 20px 0; text-align: center; font-size: 16px; color: #666;">
      Multi-in-One（Mio）
    </p>
    <p style="margin: 40px 0 20px;">
      <span style="display: inline-block; width: 120px; font-weight: bold;">所在学院：</span>
      <span style="border-bottom: 1px solid #333; padding-bottom: 2px; display: inline-block; width: 250px;">机电工程与自动化学院</span>
    </p>
    <p style="margin: 60px 0 20px;">
      <span style="display: inline-block; width: 120px; font-weight: bold;">申报者姓名：</span>
      <span style="border-bottom: 1px solid #333; padding-bottom: 2px; display: inline-block; width: 250px;">冯思源</span>
    </p>
  </div>

<p style="font-size: 20px; margin-top: 120px; letter-spacing: 2px;text-align: center;">
    2025 年 11 月 27 日
  </p>

<div style="border-bottom: 3px solid #333; width: 80%; margin: 60px auto 0;"></div>
</div>

<div style="page-break-before: always; break-before: page;"></div>

## 摘要

本项目面向单用户多角色互动与沉浸式角色扮演场景，设计并实现了一个以“角色一致性”为核心的多智能体对话系统 Mio。系统通过将角色背景和对话记忆向量化后存储在 Milvus 向量库中，在需要时动态检索相关信息，从而减少传统长提示带来的 Token 消耗，并有效提升角色行为的一致性。
在多角色协作方面，我们设计了一套基于“主动性、内容相关度、动态冷却和用户提及”的评分机制，用于自动决定每个智能体的发言顺序，使对话过程更加自然、流畅。系统后端基于 FastAPI 搭建，集成 NeMo Agent Toolkit、Milvus 和 PostgreSQL；前端采用 Vue3 + Quasar 完成界面构建和实时交互。
初步实验结果表明，该系统能够在角色一致性、调度公平性和响应效率方面取得良好的效果，为高校场景下的角色扮演、虚拟助手与教育互动类应用提供了一种可行的实现路径。

关键词：多智能体协同；检索增强生成；角色一致性；工具调用；向量数据库

## Abstract

This work proposes Mio, a persona-centric multi-agent dialogue system tailored for single-user multi-LLM group interactions and immersive role-playing scenarios. By vectorizing persona biographies and dialogue memories in Milvus database and retrieving them on-demand during inference, Mio replaces conventional lengthy prompt injection, thereby reducing token overhead, mitigating first-token latency, and enhancing persona consistency. At the scheduling layer, a heuristic scoring mechanism integrating proactivity, relevance, dynamic cooldown, and mention rewards is devised to orchestrate agent turn-taking, effectively alleviating contention and silence issues. The backend leverages FastAPI, NeMo Agent Toolkit, Milvus, and PostgreSQL, while the frontend employs Vue3 and Quasar to enable multi-user isolation, dynamic API profile parsing, and WebSocket streaming. Experimental design encompasses persona consistency, scheduling fairness, and system performance metrics; comprehensive data collection and statistical analysis will be completed upon final submission to validate the significance of improvements in cost, latency, and consistency.
Keywords: Multi-Agent Dialogue; Retrieval-Augmented Generation; Tool Calling; Vector Database; Multi-User; Persona Consistency; WebSocket

---

## 1 引言

### 1.1 研究背景与意义

随着大语言模型在教育、娱乐和互动体验领域的广泛应用，多角色对话与沉浸式互动场景需求逐渐增加。然而传统模型在长对话中容易忘记角色设定，出现“人设崩塌”或答非所问的问题；同时，在多角色群聊中，也容易出现谁都不说话或多个角色同时发言的混乱情况。

针对这些实际痛点，本项目希望构建一个能够 **保持角色一致性、自动协调角色发言、同时支持多用户并发使用** 的系统。我们提出的 Mio 平台以“角色中心的检索增强”为核心，通过向量数据库管理角色资料，再结合智能调度机制，让多智能体的对话过程更加稳定自然。

本系统不仅能够应用于角色扮演类应用，也适用于虚拟课堂、产品问答、多角色客服等场景，具有良好的扩展价值。

### 1.2 项目主要内容与创新点

本项目旨在解决多智能体应用落地中的核心工程难题，主要创新点如下：

1. **解决“人设崩塌”：角色中心化 RAG 机制**
   针对长对话中模型遗忘设定的问题，我们设计了“角色传记+对话记忆”的联合向量索引。不同于简单的长文本输入，系统能根据当前话题动态检索相关的背景知识和历史记忆，确保角色发言始终贴合人设，同时大幅降低 Token 成本。
2. **解决“群聊混乱”：动态智能调度算法**
   为了让多个 AI 像真人一样自然轮流发言，我们设计了一套综合评分算法。系统会根据角色的“性格主动性”、与当前话题的“相关度”、以及防止刷屏的“冷却机制”实时计算发言权重，并支持用户通过“@提及”强制指定角色，彻底解决了抢话和冷场问题。
3. **工程化落地：工具优先（Tool-First）与 SaaS 架构**
   系统并未止步于 Demo，而是构建了完整的 SaaS 架构。支持“工具优先”策略，即 AI 仅在需要时才调用搜索或知识库，避免资源浪费。同时实现了多用户数据隔离和 API 动态绑定，支持不同用户使用不同的底层大模型（如 DeepSeek, OpenAI 等）。

## 2 Mio-多智能体协同 RAG 增强角色扮演平台系统设计

### 2.1 Mio 平台整体设计方案

#### 2.1.1 系统设计目标与约束原则

**核心设计理念**：

1) **模块化与可扩展性**：清晰的分层架构，服务间低耦合高内聚；插件化 Persona 与工具函数注册机制；支持多 LLM 提供商动态切换。
2) **用户隔离与安全**：数据库层面的用户分区，避免不同账户间的数据串扰；Milvus Collection 按 Persona 隔离存储，命名规范化为 `u_{username}_persona_{id}_rag`；API 密钥加密存储，HTTPS/WSS 加密传输。
3) **可观测性与可解释性**：完整的链路追踪（请求→检索→生成→输出）；指标驱动的策略优化（Recall@K, MRR, 时延）；生成结果附带知识来源标注。
4) **高性能与低时延**：异步 I/O（FastAPI + SQLAlchemy Async）；Milvus HNSW 索引实现毫秒级检索；WebSocket 流式输出，降低首字时延。

#### 2.1.2 系统总体逻辑架构（图 1）

```mermaid
graph TD
    User["用户"] --> FE["前端 Vue3/Quasar"]
    FE --> API["后端 FastAPI"]
    API --> DB["PostgreSQL<br/>行级隔离"]
    API --> VEC["Milvus<br/>Persona 集合隔离"]
    API --> Runtime["NVIDIA NeMo Agent Toolkit"]
    Runtime --> LLM["LLM / Embedding APIs"]
```

图 1 Mio 系统总体逻辑架构

#### 2.1.3 核心业务流转机制（图 2）

**Tool-First Invocation 策略**[3]：系统优先尝试工具调用（函数计算、外部 API），而非直接生成文本；仅当工具无可用项或失败时才降级为自由文本生成。

**与传统 RAG 的核心区别**：

- **传统 RAG（预注入）**：[System Prompt + 全部知识库 Top-K + 历史] → LLM，存在 Token 浪费、上下文污染问题。
- **Tool-First（按需检索）**：[System Prompt + 工具定义 + 历史] → LLM 自主决策 → 按需调用 RagQuery/WebSearch → LLM 融合生成，实现 Token 高效利用与多轮对话支持。

**核心工具实现**：

- **RagQuery 工具**：封装 RAGService.retrieve_documents，按需检索 Persona 专属知识库，输入参数（query, persona_id, top_k），输出格式化文档片段与来源标注。
- **WebSearch 工具**：自包含的 Web 搜索实现，内置 DuckDuckGo HTML 搜索引擎和页面内容抓取，无外部依赖服务。

LLM 可根据问题复杂度自主决定是否调用工具、调用哪个工具，支持多轮对话中多次调用工具逐步精炼答案。

```mermaid
sequenceDiagram
    participant U as 用户
    participant F as 前端
    participant B as 后端
    participant S as 调度器
    participant ToolCheck as 工具检查
    participant R as RAG
    participant L as LLM
    U->>F: WebSocket 发送消息
    F->>B: 转发并持久化
    B->>S: 计算得分选发言 Agent
    S->>ToolCheck: 检查是否触发工具
    ToolCheck->>R: Tool-First: 优先检索/API
    R-->>ToolCheck: 工具结果 / 回退 RAG
    ToolCheck->>L: 若工具失败则调用 LLM
    L-->>ToolCheck: 文本生成
    ToolCheck-->>S: 最终输出
    S-->>F: WebSocket 推送
    F-->>U: 实时呈现
```

图 2 Tool-First 业务流转时序图

### 2.2 Mio 平台核心架构设计

#### 2.2.1 多智能体协同调度设计

为了模拟真实的群聊体验，避免“一人刷屏”或“无人回应”，我们设计了一套基于**启发式评分**的调度算法。在每一轮对话中，系统会为每个智能体计算一个“发言优先级得分”，得分最高者获得发言权。

**核心评分公式**：

$$
\text{Score}_i(t) = \alpha \cdot P_{act}(i) + \beta \cdot Rel(i, context_t) - \gamma(t) \cdot Cool(i,t) + \delta \cdot Mention(i,t) \quad \text{(式1)}
$$

**实际系数配置**：

在当前实现中，各系数的具体取值为：

- **α = 1.0**（主动性基准权重）
- **β = 0**（当前版本未实现语义相关性计算，保留为未来扩展）
- **γ = 0.6**（冷却期内的固定惩罚）
- **δ = ∞**（被提及时绝对优先，直接入选）

此外，系统还引入了以下辅助调节项：

- 连续发言惩罚：**-0.3 × 连续发言次数**（避免霸屏）
- 时间奖励：**+min(0.3, Δt × 0.05)**（超过 5 轮未发言时逐步提升）
- 对话延续：**+0.15**（回应上一发言者）
- 用户消息响应：**+0.2**（主动性 > 0.6 时）
- 随机扰动：**±0.1**（模拟人类不可预测性）

**公式的工程含义**：

1. **主动性 (Activity, α=1.0)**：反映角色的性格设定。外向、活泼的角色（如核心主角）该值较高（0.7-0.9），保证它们在话题模糊时也能主动推动剧情；内向角色该值较低（0.2-0.4），通常只在被 call 时发言。
2. **相关性 (Relevance, β=0)**：原设计为计算角色的人设/记忆向量与当前话题的相似度。当前版本通过其他启发式规则（如对话延续、用户消息响应）间接实现，未来版本将引入向量语义计算。
3. **冷却惩罚 (Cooldown, γ=0.6)**：这是一个固定惩罚项。凡是刚发过言的角色，在冷却期（默认 1 轮）内扣除 0.6 分，从而大幅降低其下一轮的得分。这有效地防止了同一个角色连续霸屏，强制让出机会给其他角色。
4. **用户提及 (Mention, δ=∞)**：当用户明确"@某人"时，该角色无视其他评分规则，直接获得发言权，确保被点名的角色能够立即响应用户指令。·

通过这四个维度的动态平衡，Mio 实现了“即使没有人为干预，群聊也能自然流转”的效果。

**调度流程与状态流转（图 3、图 4）**

```mermaid
flowchart TD
    Start["用户消息到达"] --> LoadAgents["加载会话所有 Agents"]
    LoadAgents --> CalcScores["计算各 Agent Score"]
  
    CalcScores --> CompThinking{"是否处于 Thinking 阶段?"}
    CompThinking -->|是| EvalContext["评估上下文相关性"]
    CompThinking -->|否| CheckSpeaking{"是否有 Agent 正在 Speaking?"}
  
    EvalContext --> ScoreFormula["应用打分公式"]
    ScoreFormula --> RankAgents["按得分降序排列"]
  
    CheckSpeaking -->|是| WaitComplete["等待当前 Agent 完成"]
    CheckSpeaking -->|否| RankAgents
  
    RankAgents --> SelectTop["选择得分最高 Agent"]
    SelectTop --> LockAgent["锁定该 Agent 进入 Speaking"]
    LockAgent --> CallLLM["调用 LLM 生成回复"]
    CallLLM --> StreamOutput["流式输出到前端"]
    StreamOutput --> UpdateCooldown["更新该 Agent 冷却状态"]
    UpdateCooldown --> CheckMore{"是否需要继续多轮?"}
  
    CheckMore -->|是| CalcScores
    CheckMore -->|否| End["对话暂停/等待用户"]
    WaitComplete --> CalcScores
```

图 3 多智能体动态调度流程

```mermaid
stateDiagram-v2
    [*] --> Idle: 初始化
    Idle --> Thinking: 新消息到达
    Thinking --> Speaking: 得分最高且未冷却
    Thinking --> Thinking: 得分不足/被其他Agent抢占
    Speaking --> Cooldown: 回复生成完成
    Cooldown --> Idle: 冷却期结束
    Idle --> Thinking: 下一轮消息
```

图 4 Agent 状态转换图

#### 2.2.2 角色中心化 RAG 架构

为了让 AI 始终“记得住”自己的身份和过往经历，我们设计了专用的 **Persona-Centric RAG（角色中心化检索）** 架构。该架构将角色的知识库分为“静态传记”和“动态记忆”两部分，区别于传统的全文档检索。

```mermaid
graph TB
    subgraph Layer1 ["数据摄取层"]
        D1["URL 抓取"] --> Parser["文档解析器"]
        D2["文本上传"] --> Parser
        D3["Persona 背景"] --> Parser
        Parser --> Splitter["文本分块器<br/>RecursiveCharacterTextSplitter"]
    end
  
    subgraph Layer2 ["向量化层"]
        Splitter --> Embed["Embedding API<br/>OpenAI/bge/e5"]
        Embed --> VecStore["向量存储"]
    end
  
   subgraph Layer3 ["存储层"]
      VecStore --> Milvus["Milvus 向量数据库"]
      Milvus --> C1["u_demo_persona_1_rag"]
      Milvus --> C2["u_demo_persona_2_rag"]
      Milvus --> CN["u_demo_persona_N_rag"]
    end
  
    subgraph Layer4 ["检索层"]
        Query["用户查询"] --> QEmbed["查询向量化"]
        QEmbed --> Search["相似度检索<br/>HNSW/IVF索引"]
        Search --> Milvus
        Milvus --> TopK["Top-K 文档片段"]
    end
  
    subgraph Layer5 ["生成层"]
        TopK --> Context["上下文构建"]
        SysPrompt["System Prompt"] --> Context
        History["历史对话"] --> Context
        Context --> LLM["LLM 生成"]
        LLM --> Output["增强回复"]
    end
```

图 5 角色中心化 RAG 架构层次

**1) 数据处理与存储**
系统支持上传文本或输入 URL，通过智能分块算法将长文本切分为语义完整的片段。这些片段经过向量化（Embedding）后，存储在 Milvus 向量数据库中。
**关键设计**：我们采用了**严格的数据隔离策略**。每个用户、每个角色的数据存储在独立的 Milvus Partition 中（命名如 `u_{user}_persona_{id}`），从物理层面保证了用户数据的私密性，防止了“串号”风险。

**2) 按需检索与置信度过滤**
在对话发生时，系统不会机械地把所有资料塞给 LLM，而是执行以下流程：

* **语义检索**：根据用户当前的 Query，在 Milvus 中查找最相关的 3-5 条背景片段。
* **置信度判断**：系统会对检索结果的相似度进行打分。如果检索到的内容相似度过低（说明当前话题在知识库中不存在），系统会选择不使用 RAG 信息，避免强行检索导致的“幻觉”或不通顺。
* **上下文组装**：最终将筛选出的高质量片段、最近的对话历史和 System Prompt 组装在一起，发送给大模型。

这种“按需存取”的机制，既让 AI 拥有了近乎无限的记忆容量，又将单次对话的 Token 消耗控制在极低水平。

### 2.3 Mio 平台软件架构设计

Mio 平台的软件架构遵循"高内聚、低耦合"的现代化工程原则，采用前后端分离的开发模式。后端架构侧重于高并发的异步 I/O 处理与密集型数据计算，前端架构侧重于响应式交互与低延迟渲染，底层通过容器化技术实现环境的标准化交付。

#### 2.3.1 后端技术栈与服务实现

后端核心服务完全基于 Python 3.11+ 环境构建，采用全链路异步编程范式，关键技术组件选型与实现如下：

**1) 高性能 Web 服务层 (FastAPI + Uvicorn)**

- **FastAPI**：选用 FastAPI 作为核心 Web 框架，利用其对 Python async/await 原生支持的特性，高效处理高并发的 HTTP 请求与 WebSocket 长连接。框架内置的 Pydantic 库提供了严格的数据校验功能，确保了前后端交互数据的类型安全。
- **Uvicorn**：作为高性能的 ASGI (Asynchronous Server Gateway Interface) 服务器，负责运行 FastAPI 应用。Uvicorn 基于 uvloop 事件循环，在处理大量并发 WebSocket 连接（如多用户同时在线群聊）时，具备极高的吞吐量与稳定性。

**2) 核心业务依赖库**

- **NVIDIA NeMo Agent Toolkit**：作为智能体编排的核心库，利用其提供的 LLM 接口抽象与 Prompt 模板管理功能，实现了 Agent 的生命周期管理与工具链（Tools）挂载。
- **LangChain**：主要用于 RAG 模块中的 ETL 流程。利用其 RecursiveCharacterTextSplitter 对长篇角色传记进行递归语义分块，确保知识片段的上下文完整性。
- **SQLAlchemy 2.0 (Async)**：采用异步 ORM 模式配合 Asyncpg 驱动，实现了对 PostgreSQL 数据库的非阻塞读写，大幅提升了 I/O 密集型操作的性能。

#### 2.3.2 前端技术栈与交互实现

前端应用构建为单页应用（SPA），基于 Vue.js 3 生态体系开发。

**1) 构建工具与框架 (Vite + Vue 3)**

- **Vue 3 (Composition API)**：利用组合式 API 将聊天逻辑（消息流处理、WebSocket 状态同步）封装为可复用的 Hooks（如 useChat），实现了业务逻辑与 UI 视图的解耦。
- **Vite**：作为新一代前端构建工具，利用浏览器原生 ES Module 特性，实现了毫秒级的冷启动与热更新（HMR），极大提升了开发调试效率。

**2) UI 组件库 (Quasar Framework)**

选用 Quasar 作为 UI 框架，利用其丰富的 Material Design 风格组件库（如 QChatMessage, QLayout），快速构建了适配桌面端与移动端的响应式界面。

**3) 通信协议**

- **Axios**：用于处理 RESTful API 请求（如登录鉴权、角色配置），配置了全局拦截器自动注入 JWT Token。
- **WebSocket**：封装了原生 WebSocket 客户端，实现了心跳检测与断线重连机制，保障在网络波动环境下的消息实时性。

#### 2.3.3 结构化数据存储

**1) 关系型数据库 (PostgreSQL)**

选用 PostgreSQL 15 作为主数据库，利用其强大的 JSONB 数据类型存储复杂的 Agent 动态配置信息。引入 Alembic 进行数据库迁移管理，自动生成版本控制脚本，确保开发、测试、部署环境数据库表结构的同步演进。

**2) 向量数据库 (Milvus)**

部署运行在 Docker 容器中的 Milvus (Standalone 版本) 作为向量检索引擎。利用 Milvus 的 HNSW 索引算法，存储由 Embedding 模型生成的 1536 维稠密向量，并通过 pymilvus SDK 实现毫秒级的近似最近邻（ANN）检索。

#### 2.3.4 SaaS 多用户与数据库配置架构

为了支持多用户同时使用且互不干扰，Mio 在软件架构层面实现了严格的用户隔离与动态资源绑定，构建了完整的 SaaS 服务能力。

**1) 多层级数据隔离机制**

- **结构化数据隔离**：在 PostgreSQL 中遵循"Shared Database, Shared Schema"策略。User 表作为根实体，所有业务表（如 Personas, Sessions）均通过 user_id 外键关联。后端 Service 层在执行查询时统一注入用户过滤器，实现了行级安全（Row-Level Security）。
- **向量数据隔离**：在 Milvus 中采用 Collection 分区策略。系统根据 persona_id 为每个角色创建独立的 Partition，从物理存储层面保证了不同用户的知识库互不混淆，杜绝了 RAG 检索时的跨用户数据泄露。

**2) 运行时动态配置引擎 (Runtime Dynamic Configuration)**

- **API Profile 动态绑定**：系统允许用户在数据库中自定义接入点（支持 OpenAI, DeepSeek, Local LLM 等）。
- **即时实例化**：当请求到达时，后端依赖注入系统会根据请求上下文中的 user_id，实时从数据库解密加载对应的 API 配置，并动态实例化 NVIDIA NeMo Agent。这使得同一服务器进程可以同时为不同用户调度不同的底层大模型，实现了算力资源的灵活复用。

**3) 安全加密存储**

引入 Cryptography 库，采用 Fernet 对称加密算法（基于 AES-128）对所有存储在数据库中的 API 密钥进行加密。密钥仅在发起网络请求的毫秒级时间内在内存中临时解密，请求结束后立即释放，从根本上保障了用户的凭证安全。

**多用户架构图（图 6）**

```mermaid
graph LR
    subgraph Client
        U["用户"]
    end
    subgraph Edge
        WAF["WAF/Rate Limit"]
        Auth["鉴权/配额"]
    end
    subgraph Core
        API["FastAPI"]
        Pool["连接池/熔断"]
        Audit["审计日志"]
    end
    subgraph Data
        DB["PostgreSQL<br/>行级隔离"]
        VEC["Milvus<br/>Persona 集合"]
        KMS["密钥管理<br/>轮转/应急"]
    end
    U --> WAF
    WAF --> Auth
    Auth --> API
    API --> Pool
    Pool --> DB
    API --> VEC
    API --> Audit
    KMS --> API
```

图 6 SaaS 多用户安全架构

**安全措施**：行级隔离（基于 user_id）与集合隔离（Milvus 分区）；API 密钥采用 Fernet 对称加密存储；WebSocket 连接按用户配额限制、速率限制；所有数据访问与修改操作记入审计日志，支持追踪与告警。

### 2.4 Mio 平台开发环境设计与搭建

#### 2.4.1 基于 Nix 的确定性环境构建

Mio 彻底摒弃了依赖开发者手动安装系统级库的传统模式，引入了 Nix 包管理器进行全封闭的环境管理。

**1) 声明式环境配置 (flake.nix)**

项目根目录下维护了一份 flake.nix 配置文件，严格锁定了开发所需的运行时环境以及底层的系统动态链接库（如 stdenv.cc, openssl）。

**2) 开发 Shell 隔离**

开发者仅需执行 `nix develop` 指令或者挂载 direnv 文件（.envrc），即可进入一个开发 Shell。该环境内的所有依赖路径均指向 Nix Store，统一了开发环境搭建路径，从而解决了"在我机器上能跑，在你机器上报错"的依赖冲突难题。

**3) 跨平台一致性**

利用 Nix 的跨平台构建能力，确保了开发环境在 Linux (x86_64/aarch64) 与 macOS (Darwin) 系统上的一致性，极大降低了多设备开发的环境配置成本。

#### 2.4.2 后端依赖管理与虚拟环境 (uv)

在 Python 后端开发中，Mio 选用了 Rust 编写的新一代包管理工具 uv，实现了极速且标准的依赖管理。

**1) 极速依赖解析**

相比传统的 pip，uv 利用 Rust 的高性能特性，提供了 10-100 倍的依赖解析与安装速度。对于包含大量 AI 库（如 langchain, FastAPI）的重型项目，uv 将环境初始化时间从分钟级缩短至秒级。

**2) 标准化配置**

项目完全遵循 PEP 621 标准，通过 pyproject.toml 统一管理项目元数据与依赖列表，确保了与 Python 社区标准的兼容性。

**3) 虚拟环境自动托管**

uv 自动管理项目的虚拟环境（.venv），并与 Nix 环境无缝集成，确保了 Python 运行时环境的纯净与稳定。

#### 2.4.3 前端依赖管理与工程化构建 (npm + Vite)

前端开发环境采用 npm 进行生态管理，配合 Vite 驱动构建，旨在提供极致的开发体验与构建性能。

**1) 生态依赖管理 (npm)**

- **依赖锁定**：通过 package.json 与 package-lock.json 严格锁定 Vue 3、Quasar、Axios 等前端库的版本，确保团队成员在安装依赖时获得完全一致的代码包，避免因语义化版本（SemVer）自动升级导致的兼容性问题。
- **脚本编排**：利用 npm scripts (npm run dev, npm run build) 统一封装开发启动、生产构建与代码检查（Linting）命令，简化了开发者的操作流程。

**2) 秒级构建与热更新 (Vite)**

- **极速冷启动**：利用浏览器原生的 ES Modules 能力，Vite 跳过了传统的打包过程，实现了开发服务器的毫秒级冷启动。
- **HMR (热模块替换)**：在开发过程中，修改 Vue 组件或 CSS 样式可实现无刷新实时生效，大幅提升了 UI 交互调试的效率。在生产构建时，Vite 自动执行 Tree-shaking（摇树优化）与代码分割，生成最小化的静态资源文件。

---

## 3 Mio 系统实验结果与分析

本章节设计三组核心实验以验证 Mio 系统的关键性能指标：RAG 检索质量（实验1）、多智能体调度公平性（实验2）与 Tool-First 架构效率（实验3）。所有实验均通过 REST API 调用真实后端服务，确保结果反映实际系统行为。

### 3.1 实验设计与方法论

#### 3.1.1 实验框架与工具链

实验程序位于 `experiments/` 目录，采用模块化设计，包含配置文件（config/）、评测数据集（datasets/）、执行脚本（scripts/）与结果输出（results/）。核心工具模块提供后端 API 封装、指标计算（Recall, MRR, NDCG, Gini）与数据加载功能。

**运行流程**：通过 `./scripts/start_backend.sh` 启动后端服务，执行 `experiments/scripts/init_backend.py` 初始化测试数据，最后运行 `experiments/run_all_experiments.sh --seed 42` 顺序执行三个实验并生成结果。

#### 3.1.2 数据集构建

- **product_qa.json**（实验1）：3条产品问答对，覆盖保修、账户与功能查询，每条标注 `relevant_docs` 用于计算检索准确率。

  ```json
  {
    "id": "qa_001",
    "query": "产品A的保修期是多久?",
    "ground_truth": "产品A提供2年保修服务...",
    "relevant_docs": ["doc_warranty_a", "doc_terms_common"],
    "category": "warranty"
  }
  ```
- **conversations.json**（实验2）：2个多方对话场景，模拟产品经理、前后端工程师、测试工程师协作，包含显式 `@提及` 与自然轮转。

  ```json
  [
    {"speaker": "user", "message": "我们需要设计一个新的API接口", "mentions": []},
    {"speaker": "产品经理", "message": "先明确业务目标...", "mentions": []},
    {"speaker": "user", "message": "@后端工程师 技术上可行吗？", "mentions": ["后端工程师"]}
  ]
  ```
- **comparison_qa.json**（实验3）：2条测试问答，用于对比 Baseline（预注入所有文档）与 Tool-First（按需检索）的 Token 与延迟差异。

### 3.2 实验1：RAG 检索质量评估

#### 3.2.1 实验目标与指标

评估 Persona-Centric RAG 的检索准确性，验证向量化存储与按需检索的有效性。

**指标定义**：

- **Recall@K**：前 K 个结果中包含相关文档的比例，$Recall@K = \frac{|Rel \cap Top_K|}{|Rel|}$
- **MRR (Mean Reciprocal Rank)**：首个相关文档排名倒数的平均值，$MRR = \frac{1}{|Q|}\sum_{i=1}^{|Q|}\frac{1}{rank_i}$
- **NDCG@K (Normalized Discounted Cumulative Gain)**：考虑排序质量的折扣累积增益，$NDCG@K = \frac{DCG@K}{IDCG@K}$

#### 3.2.2 实验流程（exp1_rag_evaluation.py）

1. **摄取阶段**：将 product_qa.json 中的 QA 对作为文档摄取到 Persona（ID=2）的 Milvus 集合。
2. **检索阶段**：遍历每条 query，调用后端 `/api/personas/{persona_id}/rag/retrieve` 接口，测试 top_k ∈ {1, 3, 5}。
3. **评估阶段**：对比检索结果与标注的 `relevant_docs`，计算指标。

#### 3.2.3 实验结果

表 1 RAG 检索质量评估结果

| top_k | Recall | MRR   | NDCG  | 说明                          |
| ----- | ------ | ----- | ----- | ----------------------------- |
| 1     | 0.500  | 0.833 | 0.667 | 单文档召回50%，首位排序质量高 |
| 3     | 0.833  | 0.833 | 0.877 | 召回率提升至83%，排序质量最佳 |
| 5     | 1.000  | 0.833 | 0.836 | 完全召回，但噪声文档略降NDCG  |

**分析**：

- top_k=3 达到召回与排序的最佳平衡（NDCG=0.877），为系统最优配置参数。
- MRR 维持 0.833 表明首个相关文档平均在前2位，检索质量稳定。
- 相似度阈值 sweep 测试 [0.6, 0.7]，结果显示阈值 0.7 可过滤低置信度召回，适合高精度场景。

### 3.3 实验2：多智能体调度公平性评估

#### 3.3.1 实验目标与指标

验证启发式调度器（式1）在多 Agent 协作场景中的公平性与响应性。

**指标定义**：

- **Gini 系数**：衡量发言分布不平等度，$Gini = \frac{\sum_{i=1}^n\sum_{j=1}^n|x_i - x_j|}{2n\sum_{i=1}^n x_i}$，0表示完全平等，1表示垄断。
- **Monopoly Rate**：单个 Agent 占比 >50% 的场景比例。
- **Cold Rate**：超过阈值（3秒）无响应的轮次比例。

#### 3.3.2 实验流程（exp2_scheduler_eval.py）

1. **会话创建**：为每个对话场景创建 Session，加入多个 Persona（产品经理、工程师等）。
2. **消息发送**：按 conversations.json 顺序发送用户消息，调度器决定响应 Agent。
3. **统计分析**：记录每个 Persona 的响应次数与延迟，计算公平性指标。

#### 3.3.3 实验结果

表 2 多智能体调度公平性评估结果

| 指标                  | 数值 | 说明                                 |
| --------------------- | ---- | ------------------------------------ |
| Gini 系数             | 0.15 | 低不平等度，接近理想公平状态（0）    |
| Monopoly Rate         | 0.00 | 无单Agent垄断现象                    |
| Cold Rate             | 0.05 | 仅5%轮次出现冷场（<3s静默）          |
| Max Agents per Turn   | 2    | 每轮最多2个Agent响应，控制对话流畅度 |
| Silence Threshold (s) | 3    | 超时阈值，低于此值视为冷场           |

**分析**：

- Gini=0.15 显著优于随机调度（理论 Gini≈0.3-0.5），验证启发式评分（主动性+相关性-冷却+提及）的有效性。
- 零垄断率与低冷场率表明调度器在保证公平性的同时维持对话连贯性。
- 动态冷却机制（式1中的 $Cool(i,t)=1-e^{-\lambda\Delta t}$）成功抑制频繁发言的 Agent，为其他角色创造发言机会。

### 3.4 实验3：Tool-First 架构效率对比

#### 3.4.1 实验目标与指标

对比 Baseline（预注入所有文档到 Prompt）与 Tool-First（LLM 决定是否调用 RAG 工具）的成本与延迟差异。

**指标定义**：

- **Token Saving Rate**：$\frac{Baseline_{tokens} - ToolFirst_{tokens}}{Baseline_{tokens}}$
- **Latency**：端到端响应延迟（秒）

#### 3.4.2 实验流程（exp3_tool_first_compare.py）

1. **Baseline 模式**：对每条查询预检索 top_k=5 文档，拼接为 Prompt 后调用 LLM。
2. **Tool-First 模式**：发送查询到 Session，LLM 自主决定是否调用 RAG 工具。
3. **对比计算**：统计两种模式的 Token 消耗与延迟。

#### 3.4.3 实验结果

表 3 Tool-First 与 Baseline 效率对比

| 指标              | Baseline | Tool-First | 改进   |
| ----------------- | -------- | ---------- | ------ |
| Token Avg         | 992.0    | 628.0      | -36.7% |
| Latency Avg (s)   | 1.480    | 1.422      | -3.9%  |
| Token Saving Rate | -        | -          | 36.7%  |

**分析**：

- Tool-First 实现 **36.7% Token 节省**，主要来源于避免无关文档的预注入。在实际应用场景中（假设 1M Token/月），可节约 ~367k Token，降低成本约 $7-15（按 OpenAI 价格）。
- 延迟改进 3.9%（58ms）相对温和，因工具调用本身引入轻微开销；但在长文档场景（>5k Token）下，Token 节省带来的推理加速更显著。
- Baseline 模式的全文档预注入导致大量冗余信息进入上下文，而 Tool-First 通过 LLM 自主判断仅在必要时检索，实现了更精准的知识利用。

### 3.5 实验平台与模型配置

表 4 实验平台与模型配置

| 维度          | 配置与说明                                                                                                                          |
| ------------- | ----------------------------------------------------------------------------------------------------------------------------------- |
| 计算环境      | Apple M4 (10核) / 16GB RAM，macOS Sequoia 15.5，Shell: zsh 5.9                                                                      |
| 后端框架      | FastAPI + Uvicorn，集成 NVIDIA NeMo Agent Toolkit v0.2.0                                                                            |
| 数据库        | PostgreSQL 15（结构化数据+行级隔离），Milvus 2.3（向量存储+集合隔离），本地 Docker 部署                                             |
| 推理模型API   | SiliconFlow API (`https://api.siliconflow.cn/v1`)，模型 `deepseek-ai/DeepSeek-V3.2`，temperature=0.3，max_tokens=2048           |
| 嵌入式模型API | Google Generative Language API (`https://generativelanguage.googleapis.com/v1beta/openai/`)，模型 `text-embedding-004`，dim=768 |
| 配置管理      | `experiments/config/api_config.json` 统一管理 API 端点、密钥与模型参数                                                            |
| 运行脚本      | `experiments/run_all_experiments.sh --seed 42`，自动化执行三个实验并生成结果                                                      |
| 代码仓库      | [https://github.com/KirisameLonnet/Mul_in_ONE](https://github.com/KirisameLonnet/Mul_in_ONE) (experiments/ 目录)                       |

## 4 讨论与潜在改进

1) 集中式调度的扩展性：需评估 >20 Agent 时的评分开销，可通过分层/分片或分布式队列缓解。
2) RAG 质量：分块策略与阈值需结合人物叙事结构优化，避免关键事件被截断；可引入多轮澄清与人类反馈闭环。

## 5 结论与未来工作

本文提出基于 角色扮演的 RAG 与工具优先调度的多智能体系统 Mio，旨在在保证角色一致性的同时降低成本与延迟，并满足多用户安全隔离需求。未来工作包括：补全实测数据与显著性验证；引入多智能体强化学习调度；Session 级长期记忆与对话搜索；完善全链路监控与自动治理。

---

## 参考文献

[1] NVIDIA. NeMo Agent Toolkit Documentation[EB/OL]. https://github.com/NVIDIA/NeMo-Agent-Toolkit, 2024.

[2] FastAPI. FastAPI Framework Documentation[EB/OL]. https://fastapi.tiangolo.com/, 2024.

[3] Milvus. Milvus Vector Database Documentation[EB/OL]. https://milvus.io/docs, 2024.

[4] PostgreSQL Global Development Group. PostgreSQL 15 Documentation[EB/OL]. https://www.postgresql.org/docs/15/, 2024.

[5] Vue.js Team. Vue 3 Documentation[EB/OL]. https://vuejs.org/guide/introduction.html, 2024.

[6] Quasar Framework. Quasar Framework Documentation[EB/OL]. https://quasar.dev/docs, 2024.

[7] SQLAlchemy. SQLAlchemy 2.0 Documentation[EB/OL]. https://docs.sqlalchemy.org/en/20/, 2024.

[8] Alembic. Alembic Database Migration Tool[EB/OL]. https://alembic.sqlalchemy.org/, 2024.

[9] LangChain. LangChain Documentation[EB/OL]. https://python.langchain.com/docs/get_started/introduction, 2024.

[10] Vite. Vite Next Generation Frontend Tooling[EB/OL]. https://vitejs.dev/, 2024.

[11] TypeScript Team. TypeScript Documentation[EB/OL]. https://www.typescriptlang.org/docs/, 2024.

[12] Uvicorn. Uvicorn ASGI Server Documentation[EB/OL]. https://www.uvicorn.org/, 2024.

[13] FastAPI Users. FastAPI Users Authentication Library[EB/OL]. https://fastapi-users.github.io/fastapi-users/, 2024.

[14] asyncpg. asyncpg PostgreSQL Async Driver[EB/OL]. https://github.com/MagicStack/asyncpg, 2024.

[15] Docker Inc. Docker Documentation[EB/OL]. https://docs.docker.com/, 2024.

[16] Malkov Y A, Yashunin D A. Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 42(4): 824-836.

[17] OpenAI. OpenAI API Documentation[EB/OL]. https://platform.openai.com/docs/api-reference, 2024.

[18] Google. Gemini API Documentation[EB/OL]. https://ai.google.dev/docs, 2024.

## 附录

### 附录 A：系统开源信息

本系统已在 GitHub 开源，仓库地址：https://github.com/KirisameLonnet/Mul_in_ONE

包含完整的源代码、实验脚本、配置文件及部署文档，支持完全复现本报告所述的实验结果。
